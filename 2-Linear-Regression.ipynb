{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800ea63f",
   "metadata": {},
   "source": [
    "# **Simple Linear Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b50f36",
   "metadata": {},
   "source": [
    "## **What is Simple Linear Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054ecd",
   "metadata": {},
   "source": [
    "Simple linear regression models the relationship between a single input variable $X$ and an output variable $Y$ using a straight line:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X \n",
    "$$\n",
    "\n",
    "- $Y$: the response (dependent) variable  \n",
    "- $X$: the predictor (independent) variable  \n",
    "\n",
    "---\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "- **$\\beta_0$ (Intercept)**:  \n",
    "  The expected value of $Y$ when $X = 0$. It determines where the regression line crosses the Y-axis.\n",
    "\n",
    "- **$\\beta_1$ (Slope)**:  \n",
    "  The change in the predicted value of $Y$ for a one-unit increase in $X$. It represents the strength and direction of the linear relationship between $X$ and $Y$.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions this Model can Answer\n",
    "\n",
    "1. **What is the average change in $Y$ for a one-unit increase in $X$? How Strong is the Association**  \n",
    "   → Interpreted through the slope coefficient $\\beta_1$.\n",
    "\n",
    "2. **What is the predicted value of $Y$ for a given value of $X$?**  \n",
    "   → Use the regression equation: $\\hat{Y} = \\beta_0 + \\beta_1 X$.\n",
    "\n",
    "3. **Is there a statistically significant linear relationship between $X$ and $Y$?**  \n",
    "   → Test whether $\\beta_1 \\neq 0$ using a t-test. ($H_0$)\n",
    "\n",
    "4. **How well does $X$ explain the variation in $Y$?**  \n",
    "   → Measured using the coefficient of determination, $R^2$.\n",
    "\n",
    "5. **What is the expected value of $Y$ when $X = 0$?**  \n",
    "   → Interpreted through the intercept $\\beta_0$ (if meaningful in context).\n",
    "\n",
    "6. **How much uncertainty is in our predictions?**  \n",
    "   → Use confidence intervals for $\\hat{Y}$ or prediction intervals for new observations.\n",
    "\n",
    "7. **Is there a Synergy Effect with other Predictors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab5988",
   "metadata": {},
   "source": [
    "## **Estimating Coefficients**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a532826",
   "metadata": {},
   "source": [
    "#### Residual Sum of Squares (RSS)\n",
    "\n",
    "In simple linear regression, the **residual sum of squares (RSS)** measures the total squared difference between the observed values $y_i$ and the predicted values $\\hat{y}_i$:\n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "- A **smaller RSS** means the regression line fits the data more closely.\n",
    "\n",
    "---\n",
    "\n",
    "#### Relationship Between RSS and MSE\n",
    "\n",
    "The **mean squared error (MSE)** is simply the average of RSS:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{\\text{RSS}}{n}\n",
    "$$\n",
    "\n",
    "- RSS sums the squared errors.\n",
    "- MSE scales it by the number of observations \\( n \\), giving the **average squared error**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Estimating $(\\beta_1)$ and $(\\beta_0)$\n",
    "\n",
    "To find the best-fitting line, we choose $(\\beta_0)$ and $(\\beta_1)$ that **minimize RSS**. Using calculus (taking partial derivatives of RSS and setting them to zero), we get:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "- $(\\bar{x})$ and $(\\bar{y})$ are the sample means of $(x)$ and $(y)$\n",
    "- $(\\beta_1)$ is the slope: it measures how much $(Y)$ changes per unit change in $(X)$\n",
    "- $(\\beta_0)$ is the intercept: the predicted value of $(Y)$ when $(X = 0)$\n",
    "\n",
    "These formulas result from **minimizing RSS using calculus**, by solving the normal equations derived from setting the gradients to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6d1fa",
   "metadata": {},
   "source": [
    "## **Unbiased Vs. Biased Estimators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df9f85",
   "metadata": {},
   "source": [
    "In statistics, an **estimator** is a rule or formula for estimating a population parameter (like the mean or variance) from sample data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Unbiased Estimator\n",
    "\n",
    "An estimator $\\hat{\\theta}$ is **unbiased** if its expected value equals the true parameter $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\theta}] = \\theta\n",
    "$$\n",
    "\n",
    "This means that, on average across many samples, the estimator will correctly estimate the true value.\n",
    "\n",
    "---\n",
    "\n",
    "#### Biased Estimator\n",
    "\n",
    "An estimator is **biased** if its expected value does **not** equal the true parameter:\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta \\ne 0\n",
    "$$\n",
    "\n",
    "It systematically overestimates or underestimates the true parameter.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Sample Variance\n",
    "\n",
    "- The **unbiased** sample variance uses $n - 1$ in the denominator:\n",
    "  \n",
    "  $$\n",
    "  s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n}(x_i - \\bar{x})^2\n",
    "  $$\n",
    "\n",
    "- The **biased** version uses $n$ in the denominator and underestimates the true population variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be7dab",
   "metadata": {},
   "source": [
    "## **Standard Error:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514458a7",
   "metadata": {},
   "source": [
    "The **standard error** measures the variability (or precision) of an estimator. It tells us how much the estimate would vary if we repeated the sampling process many times.\n",
    "\n",
    "---\n",
    "\n",
    "#### Variance of the Sample Mean\n",
    "\n",
    "For a sample of size $n$ from a population with variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\bar{\\mu}) = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "So the **standard error of the sample mean** is:\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\bar{\\mu}) = \\sqrt{\\frac{\\sigma^2}{n}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Standard Errors in Simple Linear Regression\n",
    "\n",
    "Let $X$ be the predictor with sample size $n$, and let $s^2$ be the residual variance estimate:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{1}{n - 2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- **Standard error of the intercept** $\\beta_0$:\n",
    "\n",
    "  $$\n",
    "  \\text{SE}(\\hat{\\beta}_0)^2 = s^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]\n",
    "  $$\n",
    "\n",
    "- **Standard error of the slope** $\\beta_1$:\n",
    "\n",
    "  $$\n",
    "  \\text{SE}(\\hat{\\beta}_1)^2 = \\frac{s^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "  $$\n",
    "  \n",
    "These quantify the uncertainty in our estimates of $\\beta_0$ and $\\beta_1$ based on the variability in the data.\n",
    "\n",
    "---\n",
    "\n",
    "Smaller standard errors indicate more precise estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92473a3",
   "metadata": {},
   "source": [
    "## **Confidence Intervals:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f048a",
   "metadata": {},
   "source": [
    "A **confidence interval (CI)** gives a range of plausible values for an unknown population parameter based on sample data. It is centered around a point estimate and constructed using the **standard error (SE)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### General Formula\n",
    "\n",
    "For a parameter estimate $\\hat{\\theta}$, a 95% confidence interval is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} \\pm z^* \\cdot \\text{SE}(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "- $\\hat{\\theta}$: point estimate (e.g., $\\hat{\\beta}_1$, $\\bar{x}$)  \n",
    "- $\\text{SE}(\\hat{\\theta})$: standard error of the estimate  \n",
    "- $z^*$: critical value from the standard normal distribution (e.g., $z^* \\approx 1.96$ for 95% confidence)\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "A 95% confidence interval means that if we repeated the sampling process many times, about **95% of the intervals** constructed this way would contain the **true population parameter**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Confidence Interval for $\\beta_1$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta}_1)\n",
    "$$\n",
    "\n",
    "This provides a plausible range for the true slope in simple linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "Note: Use a $t$-distribution (with appropriate degrees of freedom) instead of $z^*$ when the sample size is small or $\\sigma$ is unknown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7360c8f",
   "metadata": {},
   "source": [
    "## **Hypothesis Tests:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664ac88",
   "metadata": {},
   "source": [
    "In simple linear regression, you often test whether the slope $\\beta_1$ is significantly different from zero — i.e., whether the predictor $X$ has a linear relationship with the response $Y$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "- **Null hypothesis** ($H_0$): $\\beta_1 = 0$ (no linear relationship)\n",
    "- **Alternative hypothesis** ($H_1$): $\\beta_1 \\ne 0$ (there is a linear relationship)\n",
    "\n",
    "---\n",
    "\n",
    "#### Test Statistic: t-score\n",
    "\n",
    "To test $H_0$, we compute a **t-statistic**:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}\n",
    "$$\n",
    "\n",
    "- $\\hat{\\beta}_1$: estimated slope from the regression  \n",
    "- $\\text{SE}(\\hat{\\beta}_1)$: standard error of the slope\n",
    "\n",
    "This tells us how many standard errors $\\hat{\\beta}_1$ is away from zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### Using the t-Statistic in Simple Linear Regression:\n",
    "\n",
    "1. Compare the absolute value of $t$ to a **critical value** from the $t$-distribution with $n - 2$ degrees of freedom.\n",
    "2. Alternatively, calculate a **p-value** and compare it to a significance level (e.g., $\\alpha = 0.05$).\n",
    "\n",
    "- If the p-value is small (typically < 0.05), we **reject $H_0$**.\n",
    "- This suggests that the predictor $X$ is statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "A large $|t|$ value indicates strong evidence against the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340252eb",
   "metadata": {},
   "source": [
    "# **Assessing the Accuracy of the Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521ea95",
   "metadata": {},
   "source": [
    "## **Residual Standard Error:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabb1aa",
   "metadata": {},
   "source": [
    "The **Residual Standard Error (RSE)** measures the typical size of the residuals (prediction errors) in a regression model. It estimates the standard deviation of the error term $\\varepsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$\n",
    "\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n - p - 1}} = \\sqrt{\\frac{1}{n - p - 1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- $\\text{RSS}$: Residual Sum of Squares  \n",
    "- $n$: number of observations  \n",
    "- $p$: number of predictors\n",
    "- $n - 2$: degrees of freedom for simple linear regression (two parameters estimated: $\\beta_0$ and $\\beta_1$)\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- RSE gives an estimate of the **typical distance** between the actual data points and the regression line.\n",
    "- It is in the **same units as the response variable $Y$**.\n",
    "- A **smaller RSE** indicates a better fit.\n",
    "\n",
    "---\n",
    "\n",
    "#### Relation to RSS\n",
    "\n",
    "- RSS measures **total squared error**.\n",
    "- RSE adjusts RSS by dividing by degrees of freedom and taking the square root, converting it to an **average per-data-point error**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Bounds\n",
    "\n",
    "- $\\text{RSE} \\ge 0$  \n",
    "- RSE = 0 only if the model fits the data **perfectly** (all residuals = 0), which is rare and usually unrealistic.\n",
    "\n",
    "RSE is useful for comparing models or assessing absolute prediction accuracy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec28a7",
   "metadata": {},
   "source": [
    "#### Relationship Between RSE and MSE\n",
    "\n",
    "In simple linear regression, the **Residual Standard Error (RSE)** is related to the **Mean Squared Error (MSE)** as follows:\n",
    "\n",
    "$$\n",
    "\\text{RSE} = \\sqrt{\\frac{n}{n - p - 1} \\cdot \\text{MSE}}\n",
    "$$\n",
    "\n",
    "- $n$: number of observations  \n",
    "- $p$: number of predictions\n",
    "- MSE uses denominator $n$  \n",
    "- RSE uses denominator $n - 2$ to account for the degrees of freedom lost when estimating $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "---\n",
    "\n",
    "#### What They Represent\n",
    "\n",
    "- **MSE** is the **average squared residual** and is used primarily during model training to evaluate and minimize prediction error. Its how *Off* your Predictions are (Squared)\n",
    "- **RSE** is the **estimated standard deviation of the residuals**, giving an interpretable sense of the typical prediction error in the same units as $Y$. Its how *Off* your Predictions are (Relative to Scaling of Y)\n",
    "\n",
    "---\n",
    "\n",
    "So, RSE is essentially the **square root of a bias-corrected version of MSE**. When $n$ is large, the difference between RSE and $\\sqrt{\\text{MSE}}$ becomes small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd641fe",
   "metadata": {},
   "source": [
    "## **${R^2}$ Statistic:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dd7d9",
   "metadata": {},
   "source": [
    "The **$R^2$ statistic** measures the proportion of variability in the response variable $Y$ that is explained by the regression model.\n",
    "\n",
    "---\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "$$\n",
    "\n",
    "- $\\text{RSS}$: Residual Sum of Squares  \n",
    "- $\\text{TSS}$: Total Sum of Squares\n",
    "\n",
    "---\n",
    "\n",
    "#### What is TSS?\n",
    "\n",
    "$$\n",
    "\\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "- TSS measures the **total variation** in the response variable $Y$.\n",
    "- It quantifies how far the observed $y_i$ values are from the mean $\\bar{y}$.\n",
    "- TSS represents the total \"error\" you'd have if you used the mean $\\bar{y}$ to predict every $y_i$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Important: What Is a \"Good\" $R^2$?\n",
    "\n",
    "There is **no universal threshold** for a \"good\" $R^2$ — it depends on the field and type of data:\n",
    "\n",
    "- In **physics or engineering**, $R^2 > 0.9$ is common due to low-noise systems.\n",
    "- In **biology, psychology, or economics**, even $R^2$ values around 0.2–0.4 may be acceptable due to high natural variability or unobserved factors.\n",
    "- A low $R^2$ does **not always mean** the model is useless — it may still reveal important patterns or predictors.\n",
    "\n",
    "Always interpret $R^2$ **in context**, alongside domain knowledge and other model diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "$R^2$ answers the question:  \n",
    "**“How much better is my model at predicting $Y$ compared to just using the mean?”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad838468",
   "metadata": {},
   "source": [
    "## **Correlation Coefficient $(r)$:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93884f6",
   "metadata": {},
   "source": [
    "The **correlation coefficient** $r$ measures the **strength and direction** of the linear relationship between two variables $X$ and $Y$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Formula for Pearson Correlation\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "- $r$ ranges from **–1 to 1**\n",
    "  - $r = 1$: perfect positive linear relationship  \n",
    "  - $r = -1$: perfect negative linear relationship  \n",
    "  - $r = 0$: no linear relationship\n",
    "\n",
    "---\n",
    "\n",
    "#### Relationship to $R^2$\n",
    "\n",
    "In **simple linear regression** (only one predictor):\n",
    "\n",
    "$$\n",
    "R^2 = r^2\n",
    "$$\n",
    "\n",
    "- $R^2$ is the **square of the correlation** between $X$ and $Y$.\n",
    "- This means $R^2$ captures the **proportion of variance in $Y$** that can be explained by a **linear** relationship with $X$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Notes\n",
    "\n",
    "- $r$ captures **direction and strength** of linear correlation.\n",
    "- $R^2$ captures **how much variation** in $Y$ is explained — it is **always positive**.\n",
    "- In multiple regression, $R^2$ is **not equal** to the square of any single $r$ — it accounts for **joint effects** of all predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc9845",
   "metadata": {},
   "source": [
    "# **Multiple Linear Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7023e9",
   "metadata": {},
   "source": [
    "## **Why Multiple Linear Regression?:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30851d6f",
   "metadata": {},
   "source": [
    "**Multiple linear regression** models the relationship between several input variables $X_1, X_2, \\dots, X_p$ and an output variable $Y$ using a linear equation:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n",
    "$$\n",
    "\n",
    "- $Y$: the response (dependent) variable  \n",
    "- $X_1, X_2, \\dots, X_p$: predictor (independent) variables  \n",
    "\n",
    "---\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "- **$\\beta_0$ (Intercept)**:  \n",
    "  The expected value of $Y$ when all predictors are zero.\n",
    "\n",
    "- **$\\beta_j$ (Coefficient for $X_j$)**:  \n",
    "  The change in the predicted value of $Y$ for a one-unit increase in $X_j$, holding all other predictors constant.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions This Model Can Answer\n",
    "\n",
    "1. **What is the expected change in $Y$ for a one-unit increase in $X_j$, controlling for other predictors?**  \n",
    "   → Interpreted through $\\beta_j$.\n",
    "\n",
    "2. **What is the predicted value of $Y$ for a given combination of predictors?**  \n",
    "   → Use the regression equation: $\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p$.\n",
    "\n",
    "3. **Is there a statistically significant relationship between each predictor and $Y$?**  \n",
    "   → Test whether each $\\beta_j \\ne 0$ using individual t-tests ($H_0: \\beta_j = 0$).\n",
    "\n",
    "4. **How well do all predictors together explain the variation in $Y$?**  \n",
    "   → Measured using the coefficient of determination, $R^2$ (or adjusted $R^2$ for multiple predictors).\n",
    "\n",
    "5. **What is the expected value of $Y$ when all $X_j = 0$?**  \n",
    "   → Interpreted through $\\beta_0$ (context-dependent).\n",
    "\n",
    "6. **How much uncertainty is in our predictions?**  \n",
    "   → Use confidence intervals for $\\hat{Y}$ or prediction intervals for new observations.\n",
    "\n",
    "7. **Is there a synergy or interaction between predictors?**  \n",
    "   → Include and test interaction terms (e.g., $X_1 \\cdot X_2$) in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56696977",
   "metadata": {},
   "source": [
    "## **F Statistic**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af4732",
   "metadata": {},
   "source": [
    "The **F-statistic** is used to test the **overall significance** of a linear regression model. It evaluates whether at least one predictor variable has a non-zero coefficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "While a **t-test** evaluates the significance of a **single predictor** ($H_0: \\beta_j = 0$), the **F-test** answers:\n",
    "\n",
    "> **Is the model better than a model with no predictors at all?**  \n",
    "> That is, **does at least one $\\beta_j \\ne 0$?**\n",
    "\n",
    "---\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "- **Null hypothesis ($H_0$):** All slope coefficients are zero: $\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$\n",
    "- **Alternative hypothesis ($H_1$):** At least one $\\beta_j \\ne 0$\n",
    "\n",
    "---\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{Explained variance per predictor}}{\\text{Unexplained variance per residual}} = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)}\n",
    "$$\n",
    "\n",
    "- $TSS$: Total Sum of Squares  \n",
    "- $RSS$: Residual Sum of Squares  \n",
    "- $p$: number of predictors  \n",
    "- $n$: number of observations\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- A **large F-value** indicates that the model explains a significant amount of variability in $Y$ compared to a null model (with only the intercept).\n",
    "- The associated **p-value** tells you if this improvement is statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Use F Instead of t or z?\n",
    "\n",
    "- In **multiple regression**, using **individual t-tests** for each predictor can miss the **combined effect** of variables.\n",
    "- The **F-test** evaluates the model **as a whole** and is especially important when:\n",
    "  - Testing overall model fit\n",
    "  - Performing **ANOVA**\n",
    "  - Comparing nested models\n",
    "\n",
    "---\n",
    "\n",
    "In summary, the F-statistic tests whether your model has **any predictive power at all**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183c7ad",
   "metadata": {},
   "source": [
    "#### F-Statistic for a Subset of Predictors\n",
    "\n",
    "The **F-statistic** can be used to test whether a **subset of predictors** contributes significantly to explaining the variation in $Y$, beyond what is explained by the rest of the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### Use Case: Comparing Two Models\n",
    "\n",
    "We compare:\n",
    "- A **reduced model** (without certain predictors)\n",
    "- A **full model** (with all predictors, including those being tested)\n",
    "\n",
    "**Goal:** Determine if the additional predictors in the full model provide a **statistically significant improvement**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "- **$H_0$ (null):** The additional predictors have **no effect**; their coefficients are all zero.\n",
    "- **$H_1$ (alternative):** At least one of the added predictors has a **non-zero** effect.\n",
    "\n",
    "---\n",
    "\n",
    "#### F-Statistic for Partial Effect\n",
    "\n",
    "$$\n",
    "F = \\frac{(RSS_{\\text{reduced}} - RSS_{\\text{full}}) / q}{RSS_{\\text{full}} / (n - p - 1)}\n",
    "$$\n",
    "\n",
    "- $RSS_{\\text{reduced}}$: RSS from the smaller model  \n",
    "- $RSS_{\\text{full}}$: RSS from the larger model  \n",
    "- $q$: number of predictors added (difference in model size)  \n",
    "- $p$: total number of predictors in the full model  \n",
    "- $n$: number of observations\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- A **large F-value** suggests that the subset of predictors being tested **improves the model fit** significantly.\n",
    "- The corresponding **p-value** tells you whether this improvement is **statistically significant**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "- This form of the F-test reveals the **partial effect** of adding new variables.\n",
    "- It helps determine whether adding variables is **justified**, or if they contribute **redundant or noise-driven information**.\n",
    "\n",
    "---\n",
    "\n",
    "This method is often used in **model selection**, **stepwise regression**, and **testing interaction effects**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a72c",
   "metadata": {},
   "source": [
    "## **Variable Selection in Linear Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf2df5",
   "metadata": {},
   "source": [
    "In multiple linear regression, we often have many potential predictors. **Variable selection** is the process of choosing a subset of those predictors that best explain the response variable $Y$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Variable Selection Matters\n",
    "\n",
    "- Including **too many predictors** can lead to **overfitting**, increased variance, and less interpretability.\n",
    "- Including **irrelevant variables** can dilute the effect of important ones.\n",
    "- Fewer, well-chosen predictors lead to **simpler, more robust, and more interpretable models**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Not Try Every Possible Model?\n",
    "\n",
    "- For $p$ predictors, there are $2^p$ possible subsets.  \n",
    "  → For just 20 variables, that's over 1 million models.\n",
    "- **Exhaustive search** is computationally infeasible for even moderately sized problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Variable Selection Methods\n",
    "\n",
    "#### 1. Forward Selection\n",
    "\n",
    "- Start with no predictors.\n",
    "- Add the predictor that improves model fit the most (e.g., lowest AIC/BIC, p-value, highest adjusted $R^2$).\n",
    "- Repeat until adding predictors no longer improves the model significantly.\n",
    "\n",
    "**Best for**: smaller datasets where starting from a minimal model makes sense.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Backward Elimination\n",
    "\n",
    "- Start with all predictors.\n",
    "- Iteratively remove the least significant predictor (e.g., highest p-value).\n",
    "- Stop when all remaining variables are statistically significant.\n",
    "\n",
    "**Best for**: situations where $n > p$ and you suspect many irrelevant variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Stepwise (Mixed) Selection\n",
    "\n",
    "- Combines forward and backward steps.\n",
    "- At each step, you can add or remove a variable based on a criterion (like AIC or p-value).\n",
    "\n",
    "**Best for**: flexible balance between the two methods when you're unsure of variable importance.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Method            | Use Case                                      |\n",
    "|-------------------|-----------------------------------------------|\n",
    "| Forward Selection | Few predictors expected to be relevant        |\n",
    "| Backward Elimination | Many predictors, want to simplify            |\n",
    "| Stepwise Selection| Need automation or compromise between both    |\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications of Variable Selection\n",
    "\n",
    "1. **Healthcare**: Identify key risk factors for disease from dozens of clinical indicators.\n",
    "2. **Finance**: Select relevant macroeconomic variables to predict asset returns.\n",
    "3. **Marketing**: Choose customer attributes that drive purchasing behavior.\n",
    "4. **Manufacturing**: Predict product quality based on process variables.\n",
    "5. **Energy**: Forecast power consumption using selected environmental and usage features.\n",
    "\n",
    "---\n",
    "\n",
    "Variable selection improves **model performance**, **interpretability**, and **generalization** — making it essential in applied data science and decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112386b2",
   "metadata": {},
   "source": [
    "## **Qualitative Predictors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a90ff",
   "metadata": {},
   "source": [
    "Linear regression can include **categorical variables** like homeownership status by converting them into **dummy variables**. Let’s say we're modeling **average credit card balance ($Y$)** based on whether someone **owns a home**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Binary (2-Level) Categorical Predictor: Homeownership\n",
    "\n",
    "Suppose we have a variable `Homeowner` with two levels: **Yes** and **No**. We encode it as a dummy variable:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{cases}\n",
    "1 & \\text{if Homeowner = Yes} \\\\\n",
    "0 & \\text{if Homeowner = No}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The regression model becomes:\n",
    "\n",
    "$$\n",
    "\\text{Balance} = \\beta_0 + \\beta_1 X + \\varepsilon\n",
    "$$\n",
    "\n",
    "- **$\\beta_0$**: the **average balance** for non-homeowners ($X = 0$)  \n",
    "- **$\\beta_1$**: the **difference** in average balance between homeowners and non-homeowners  \n",
    "  → So, average balance for homeowners = $\\beta_0 + \\beta_1$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Multi-Level Example: Housing Type\n",
    "\n",
    "Suppose instead of just \"Homeowner\", we have a variable `Housing` with 3 categories: **Renter**, **Owner**, **Living with Parents**. We create 2 dummy variables:\n",
    "\n",
    "- $X_1 = 1$ if Owner, 0 otherwise  \n",
    "- $X_2 = 1$ if Living with Parents, 0 otherwise  \n",
    "- Renter is the **baseline**\n",
    "\n",
    "The model becomes:\n",
    "\n",
    "$$\n",
    "\\text{Balance} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "- **$\\beta_0$**: average balance for **Renters**  \n",
    "- **$\\beta_1$**: difference in average balance between **Owners** and Renters  \n",
    "- **$\\beta_2$**: difference in average balance between those **Living with Parents** and Renters\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Including categorical variables allows us to assess **how group membership affects the response**.\n",
    "- The coefficients tell us the **average differences** in credit card balance **relative to a baseline group**.\n",
    "- This helps identify financial behavior patterns across groups (e.g., homeowners may carry lower balances).\n",
    "\n",
    "---\n",
    "\n",
    "This is how we bring qualitative traits into a quantitative model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24432ecf",
   "metadata": {},
   "source": [
    "# **Extending the Linear Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f791ae8",
   "metadata": {},
   "source": [
    "## **Removing the Additive Assumption:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21419a34",
   "metadata": {},
   "source": [
    "Linear regression makes two key assumptions about the relationship between predictors and the response:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Linearity** Assumption\n",
    "\n",
    "The relationship between each predictor $X_j$ and the response $Y$ is **linear**:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon\n",
    "$$\n",
    "\n",
    "This means each predictor contributes to $Y$ **proportionally** and independently. The effect of $X_j$ on $Y$ is constant — regardless of the values of other predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Additivity** Assumption\n",
    "\n",
    "Each predictor’s effect is **added** to the total prediction:\n",
    "\n",
    "- No interaction or synergy is assumed between predictors.\n",
    "- For example, $\\beta_1 X_1$ affects $Y$ the same way whether $X_2$ is low or high.\n",
    "\n",
    "---\n",
    "\n",
    "### Going Beyond Additivity: Modeling Synergy or Interference\n",
    "\n",
    "In real-world data, predictors may **interact** — the effect of one depends on the level of another. This is common in:\n",
    "\n",
    "- **Medicine**: two treatments may work better together (synergy) or cancel each other (interference)\n",
    "- **Marketing**: a discount and an ad campaign may only work well **together**\n",
    "- **Manufacturing**: pressure and temperature may jointly affect material strength\n",
    "\n",
    "---\n",
    "\n",
    "#### Capturing Interactions\n",
    "\n",
    "To allow for non-additive effects, include **interaction terms**:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\cdot X_2) + \\varepsilon\n",
    "$$\n",
    "\n",
    "- The term $\\beta_3 (X_1 \\cdot X_2)$ models the **interaction** between $X_1$ and $X_2$\n",
    "- Now, the effect of $X_1$ on $Y$ **depends on the value of $X_2$**, and vice versa\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Linearity** assumes straight-line effects.\n",
    "- **Additivity** assumes independent contributions.\n",
    "- **Interaction terms** break additivity and allow the model to capture **synergies or interferences** between predictors.\n",
    "\n",
    "This makes the model more flexible and often more realistic — at the cost of increased complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbc236",
   "metadata": {},
   "source": [
    "#### Example: Predicting Productivity in Manufacturing\n",
    "\n",
    "Suppose we want to model **productivity** (e.g., units produced per shift) in a factory. Two important predictors might be:\n",
    "\n",
    "- $X_1$: **Number of workers on the line**\n",
    "- $X_2$: **Number of manufacturing lines operating**\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Linear Additive Model\n",
    "\n",
    "We start with a simple linear regression:\n",
    "\n",
    "$$\n",
    "\\text{Productivity} = \\beta_0 + \\beta_1 \\cdot \\text{Workers} + \\beta_2 \\cdot \\text{Lines} + \\varepsilon\n",
    "$$\n",
    "\n",
    "This assumes:\n",
    "- Each **additional worker** increases productivity by $\\beta_1$ units, regardless of how many lines are running.\n",
    "- Each **additional line** increases productivity by $\\beta_2$ units, regardless of how many workers there are.\n",
    "\n",
    "**Limitation**: This ignores **synergy** — the fact that adding more workers may only help if more lines are active (and vice versa).\n",
    "\n",
    "---\n",
    "\n",
    "### Adding an Interaction Term (Synergy)\n",
    "\n",
    "To capture this dependency, we introduce an interaction term:\n",
    "\n",
    "$$\n",
    "\\text{Productivity} = \\beta_0 + \\beta_1 \\cdot \\text{Workers} + \\beta_2 \\cdot \\text{Lines} + \\beta_3 \\cdot (\\text{Workers} \\cdot \\text{Lines}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "- $\\beta_3$ captures the **synergy** between_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c02bb5",
   "metadata": {},
   "source": [
    "## **Non-Linear Relatinships:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77a244",
   "metadata": {},
   "source": [
    "Linear regression assumes a **linear relationship** between each predictor and the response. But in many real-world cases, the effect of a variable on the outcome is **nonlinear**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Predicting MPG with Horsepower\n",
    "\n",
    "Suppose we want to predict a car’s **fuel efficiency (MPG)** using its **horsepower**:\n",
    "\n",
    "**Initial model:**\n",
    "\n",
    "$$\n",
    "\\text{MPG} = \\beta_0 + \\beta_1 \\cdot \\text{Horsepower} + \\varepsilon\n",
    "$$\n",
    "\n",
    "This model assumes:\n",
    "- Each additional unit of horsepower decreases MPG by a **constant amount** ($\\beta_1$).\n",
    "- The relationship is a **straight line**.\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem: The Relationship May Be Curved\n",
    "\n",
    "In reality:\n",
    "- Going from 100 → 120 HP may reduce MPG slightly.\n",
    "- Going from 300 → 320 HP might reduce MPG **much more**.\n",
    "\n",
    "This suggests a **nonlinear** relationship — MPG drops **faster** at higher horsepower levels.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Nonlinearity with Polynomial Terms\n",
    "\n",
    "We can capture this curvature by adding a **squared term**:\n",
    "\n",
    "$$\n",
    "\\text{MPG} = \\beta_0 + \\beta_1 \\cdot \\text{Horsepower} + \\beta_2 \\cdot \\text{Horsepower}^2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "- This is still a **linear model** in terms of parameters (so we can use linear regression techniques).\n",
    "- But it allows the predicted MPG curve to **bend** — typically downward in this case.\n",
    "- $\\beta_2$ controls the **curvature** (e.g., negative for downward bend).\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Helps\n",
    "\n",
    "- More flexible models can capture realistic behavior.\n",
    "- Polynomial terms are a simple way to model nonlinearity **without switching to complex models**.\n",
    "\n",
    "This is a common technique in regression diagnostics and model refinement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09847d",
   "metadata": {},
   "source": [
    "# **Potential Problems with Linear Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656de85",
   "metadata": {},
   "source": [
    "## **Non-Linearity of the Data:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fff8e",
   "metadata": {},
   "source": [
    "## **Correlation of Error Terms:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79c15b",
   "metadata": {},
   "source": [
    "## **Non-Constant Variance of Error Terms:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633e115",
   "metadata": {},
   "source": [
    "## **Outliers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06d79c",
   "metadata": {},
   "source": [
    "## **High-Leverage Points:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce86ac",
   "metadata": {},
   "source": [
    "## **Collinearity:**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
