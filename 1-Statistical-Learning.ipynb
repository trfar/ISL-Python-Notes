{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f548c03",
   "metadata": {},
   "source": [
    "# **Introduction of Statistical Modeling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab2e90",
   "metadata": {},
   "source": [
    "## **Vocabulary:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74078c",
   "metadata": {},
   "source": [
    "${Y}$ = True Response Value \n",
    "\n",
    "$\\hat{Y}$  = Model Prediction Value\n",
    "\n",
    "$\\varepsilon$ = Error Term\n",
    "\n",
    "$E$ = Expected Value or Average Value\n",
    "\n",
    "$B_n$ = Model Parameter or *Weight*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36d599",
   "metadata": {},
   "source": [
    "## **Inference Vs Prediction Modeling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab6aba",
   "metadata": {},
   "source": [
    "### Inference Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873e3a4",
   "metadata": {},
   "source": [
    "Understanding the Association between a Response $Y$ and its Predictors $X_n$ . \n",
    "\n",
    "- Which Predictors are Associated wiht the Response\n",
    "- What is the Relationship between the Response and **Each** Predictor\n",
    "- How complicated is the Relationship? (Linear or Non-Linear) (Model Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c95fe4",
   "metadata": {},
   "source": [
    "### Prediction Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16512269",
   "metadata": {},
   "source": [
    "Understanding How Well We Can Predict $Y$ from $X_n$\n",
    "\n",
    "- How accurately can we predict $Y$ from new observations of $X_n$?\n",
    "- What combination of predictors gives the best prediction?\n",
    "- How well does the model generalize to unseen data?\n",
    "- What is the trade-off between bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a415",
   "metadata": {},
   "source": [
    "## **Reducible vs Irreducible Error:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67842200",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\mathbb{E}(Y - \\hat{Y}) = \\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(X))^2]}_{\\text{Reducible error}} + \\underbrace{\\operatorname{Var}(\\varepsilon)}_{\\text{Irreducible error}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b31a6a",
   "metadata": {},
   "source": [
    "Machine Learning Approaches Reduce Error by Applying Models that more Accurately Capture Underlying Relationships. This is Considered Reducible Error. Additionally, the Effectiveness of a Model Capturing the True Relationship is Considered Model Bias\n",
    "\n",
    "However if $\\hat{f}$ Perfectly Estimated ${f}$, there would still be Error due to Inherent Variability ($\\varepsilon$). This is Considered Irreducible Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd086fd",
   "metadata": {},
   "source": [
    "## **Parametric Vs Non-Parametric Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e018b",
   "metadata": {},
   "source": [
    "### **Parametric Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5afee3",
   "metadata": {},
   "source": [
    "#### Assume the Form of the Function ${f}$ and Summarize the Data using a Fixed Number of Parameters\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use:\n",
    "\n",
    "- When you have **strong prior knowledge** or assumptions about the data structure.\n",
    "- When **interpretability** and **efficiency** are important.\n",
    "- With **small to moderate** data where overfitting is a concern.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Models:\n",
    "\n",
    "- Linear/Logistic Regression\n",
    "- Ridge/Lasso Regression\n",
    "- Naive Bayes (With Fixed Distribution)\n",
    "- ARIMA (in Time Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d5f65",
   "metadata": {},
   "source": [
    "### **Non-Parametric Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff46fe",
   "metadata": {},
   "source": [
    "#### Make no Strong Assumption about the Form of ${f}$ and let the Data Determine the Model Complexity (Parameters Grow with Data)\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use:\n",
    "\n",
    "- When the true relationship is **unknown or highly complex**.\n",
    "- When you have **large datasets** that can support more **flexible** models.\n",
    "- When **prediction accuracy** is more important than interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Models:\n",
    "- K Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "- Splines\n",
    "- Random Forests\n",
    "- Neural Networks/ Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285750f1",
   "metadata": {},
   "source": [
    "## **Supervised vs Unsupervised Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8e4ad",
   "metadata": {},
   "source": [
    "### **Supervised Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0fb51",
   "metadata": {},
   "source": [
    "#### What Makes It Supervised?\n",
    "\n",
    "- The model is trained on a labeled dataset, where each input $X$ is paired with a known output $Y$.\n",
    "- The goal is to **learn a mapping** from inputs to outputs: $f: X \\rightarrow Y$.\n",
    "- Performance is measured using the difference between predicted and true labels (e.g., accuracy, MSE).\n",
    "\n",
    "---\n",
    "#### Industry Applications & Problems\n",
    "\n",
    "1. **Healthcare**: Predicting disease risk (e.g., diabetes or cancer diagnosis from patient data)\n",
    "2. **Finance**: Credit scoring and loan default prediction\n",
    "3. **Retail & Commerce**: Forecasting sales or predicting customer churn\n",
    "\n",
    "---\n",
    "#### Common Supervised Learning Algorithms\n",
    "\n",
    "1. **Linear Regression** – for continuous outputs\n",
    "2. **Logistic Regression** – for binary classification\n",
    "3. **Support Vector Machines (SVM)**\n",
    "4. **Random Forests**\n",
    "5. **Neural Networks** (for regression and classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcef11",
   "metadata": {},
   "source": [
    "### **Unsupervised Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb7526",
   "metadata": {},
   "source": [
    "#### What Makes It Unsupervised?\n",
    "\n",
    "- The model is trained on data **without labels** — only input $X$ is observed.\n",
    "- The goal is to **discover hidden patterns, structure, or groupings** in the data.\n",
    "- There is no explicit “correct” output to compare to — evaluation is task-specific.\n",
    "\n",
    "---\n",
    "\n",
    "#### Industry Applications & Problems\n",
    "\n",
    "1. **Banking**: Anomaly detection for fraud or unusual transactions\n",
    "2. **E-commerce**: Identifying consumer behavior segments for targeted marketing\n",
    "3. **Cybersecurity**: Detecting unusual network traffic or attacks\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Unsupervised Learning Algorithms\n",
    "\n",
    "1. **K-Means Clustering**\n",
    "2. **Hierarchical Clustering**\n",
    "3. **Principal Component Analysis (PCA)**\n",
    "4. **Autoencoders**\n",
    "5. **GMM (Gaussian Mixture Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a8066",
   "metadata": {},
   "source": [
    "## **Regression Vs. Classification:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395dab3",
   "metadata": {},
   "source": [
    "### **Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe05366",
   "metadata": {},
   "source": [
    "#### What Makes It Regression?\n",
    "\n",
    "- The **output variable $Y$ is continuous** — it can take any real value within a range.\n",
    "- The goal is to **predict a numeric quantity** based on input features $X$.\n",
    "- Evaluation metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and $R^2$ score.\n",
    "\n",
    "---\n",
    "\n",
    "#### Industry Applications & Problems\n",
    "\n",
    "1. **Real Estate**: Predicting house prices based on location, size, and features  \n",
    "2. **Finance**: Forecasting stock prices or market returns  \n",
    "3. **Healthcare**: Estimating patient survival time or disease progression  \n",
    "\n",
    "---\n",
    "\n",
    "#### Common Regression Algorithms\n",
    "\n",
    "1. Linear Regression  \n",
    "2. Ridge / Lasso Regression  \n",
    "3. Decision Trees for Regression  \n",
    "4. Random Forest Regressor  \n",
    "5. Gradient Boosting Regressor (e.g., XGBoost, LightGBM)  \n",
    "6. Support Vector Regression (SVR)  \n",
    "7. k-Nearest Neighbors (kNN) Regressor  \n",
    "8. Neural Networks (for continuous output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c96ad",
   "metadata": {},
   "source": [
    "### **Classification:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74f917",
   "metadata": {},
   "source": [
    "#### What Makes It Classification?\n",
    "\n",
    "- The **output variable $Y$ is categorical** — it belongs to a finite set of classes or labels.\n",
    "- The goal is to **assign input $X$ to one of the predefined classes**.\n",
    "- Evaluation metrics include Accuracy, Precision, Recall, F1 Score, and AUC-ROC.\n",
    "\n",
    "---\n",
    "\n",
    "#### Industry Applications & Problems\n",
    "\n",
    "1. **Healthcare**: Diagnosing disease (e.g., predicting if a tumor is malignant or benign)  \n",
    "2. **Email Filtering**: Classifying spam vs. non-spam messages  \n",
    "3. **Customer Analytics**: Predicting if a user will churn or renew  \n",
    "\n",
    "---\n",
    "\n",
    "#### Common Classification Algorithms\n",
    "\n",
    "1. Logistic Regression  \n",
    "2. Decision Trees for Classification  \n",
    "3. Random Forest Classifier  \n",
    "4. Support Vector Machines (SVM)  \n",
    "5. k-Nearest Neighbors (kNN) Classifier  \n",
    "6. Naive Bayes  \n",
    "7. Gradient Boosting Classifier (e.g., XGBoost, LightGBM)  \n",
    "8. Neural Networks (for classification tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc433780",
   "metadata": {},
   "source": [
    "# **Measuring the Quality of a Models Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bf55e",
   "metadata": {},
   "source": [
    "## **Mean Squared Error(MSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcf2cd",
   "metadata": {},
   "source": [
    "The **Mean Squared Error (MSE)** is a common loss function used to evaluate regression models. It measures the average of the squared differences between the actual values $Y$ and the predicted values $\\hat{Y}$.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- $y_i$ is the true value for the $i^{th}$ data point  \n",
    "- $\\hat{y}_i$ is the predicted value  \n",
    "- $n$ is the total number of observations\n",
    "\n",
    "---\n",
    "\n",
    "#### How MSE Relates to Overfitting\n",
    "\n",
    "- A **low training MSE** means the model fits the training data well.\n",
    "- However, if the **test MSE starts increasing while training MSE continues to decrease**, the model is likely **overfitting** — it’s learning patterns and noise specific to the training set that do not generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fee98a",
   "metadata": {},
   "source": [
    "## **Bias-Variance Trade Off**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e46db",
   "metadata": {},
   "source": [
    "In supervised learning, the expected squared prediction error at a given input $X$ can be decomposed into three components:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\underbrace{[\\text{Bias}(\\hat{f}(X))]^2}_{\\text{Bias}^2} + \\underbrace{\\text{Var}(\\hat{f}(X))}_{\\text{Variance}} + \\underbrace{\\operatorname{Var}(\\varepsilon)}_{\\\\text{Irreducible error}}\n",
    "$$\n",
    "\n",
    "- **Bias** measures the error introduced by approximating a complex real-world problem with a simplified model. It reflects how far the average prediction $\\mathbb{E}[\\hat{f}(X)]$ is from the true function $f(X)$.\n",
    "  \n",
    "- **Variance** measures how much the model's prediction $\\hat{f}(X)$ would change if it were trained on a different dataset. High variance means the model is sensitive to fluctuations in the training data.\n",
    "\n",
    "- **Irreducible error** ($\\operatorname{Var}(\\varepsilon)$) represents the inherent noise in the data — variation in $Y$ that cannot be explained even if we knew the true function $f(X)$ perfectly.\n",
    "\n",
    "The trade-off lies in managing bias and variance: improving one often worsens the other. A good model finds the right balance to minimize the overall prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f15b42",
   "metadata": {},
   "source": [
    "## **Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2ae82",
   "metadata": {},
   "source": [
    "The **Bayes classifier** is a theoretical model that assigns a new observation $X = x$ to the class $k$ that has the **highest probability of being correct**, given the observed input:\n",
    "\n",
    "$$\n",
    "\\text{Class}(x) = \\arg\\max_k \\ \\mathbb{P}(Y = k \\mid X = x)\n",
    "$$\n",
    "\n",
    "This probability — $\\mathbb{P}(Y = k \\mid X = x)$ — is called the **posterior probability**, meaning:\n",
    "> \"Given what I've observed (input $x$), how likely is it that this data point belongs to class $k$?\"\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes’ Theorem Behind the Scenes\n",
    "\n",
    "The posterior is computed using **Bayes’ Theorem**:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y = k \\mid X = x) = \\frac{\\mathbb{P}(X = x \\mid Y = k) \\cdot \\mathbb{P}(Y = k)}{\\mathbb{P}(X = x)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Posterior**: $\\mathbb{P}(Y = k \\mid X = x)$ — the probability of class $k$ given input $x$\n",
    "- **Likelihood**: $\\mathbb{P}(X = x \\mid Y = k)$ — how likely input $x$ is under class $k$\n",
    "- **Prior**: $\\mathbb{P}(Y = k)$ — how common class $k$ is before seeing any data\n",
    "- **Evidence**: $\\mathbb{P}(X = x)$ — the overall probability of observing $x$ (used for normalization)\n",
    "\n",
    "Think of it like this:\n",
    "- The **prior** is your starting belief about which class is likely, *before* you look at the data.\n",
    "- The **likelihood** tells you how well the observed data $x$ fits each possible class.\n",
    "- The **posterior** combines both: it updates your belief about the class *after* seeing the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Why the Bayes Classifier Matters — and Why It’s Hard to Use\n",
    "\n",
    "- It is **theoretically optimal**: No classifier can achieve a lower average error than the Bayes classifier if you knew the true distributions.\n",
    "- But it's **not practical in real life**, because:\n",
    "  - We **don’t know** the true distributions $\\mathbb{P}(X \\mid Y)$.\n",
    "  - Estimating these distributions perfectly would require **infinite data**, or unrealistic assumptions.\n",
    "  - Computing exact probabilities in high-dimensional space is **computationally expensive or impossible**.\n",
    "\n",
    "---\n",
    "\n",
    "### What We Do Instead\n",
    "\n",
    "In practice, we approximate the Bayes classifier using simpler or more flexible models:\n",
    "- **Naive Bayes** assumes features are conditionally independent\n",
    "- **Linear Discriminant Analysis (LDA)** assumes normal distributions with equal variance\n",
    "- **Non-parametric methods** like k-NN use data points directly to estimate probabilities\n",
    "\n",
    "The Bayes classifier acts as a **gold standard** — a benchmark for how well our models could perform in the best-case scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023eb8c2",
   "metadata": {},
   "source": [
    "## **K-Nearest Neighbors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5af5d4",
   "metadata": {},
   "source": [
    "**k-NN** is a non-parametric method used for classification and regression. It makes predictions based on the $k$ closest training points to a new input $x$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Classification Prediction Rule\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{j \\in \\mathcal{C}} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{I}(y_i = j)\n",
    "$$\n",
    "\n",
    "- $\\mathcal{C}$: set of possible classes  \n",
    "- $\\mathcal{N}_k(x)$: indices of the $k$ nearest training points to $x$  \n",
    "- $\\mathbb{I}(y_i = j)$: Indicator Function (1 if the $i$-th neighbor belongs to class $j$, 0 otherwise)\n",
    "\n",
    "---\n",
    "\n",
    "#### Class Probability Estimate\n",
    "\n",
    "The estimated probability that $Y = j$ given input $X = x$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y = j \\mid X = x) \\approx \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{I}(y_i = j)\n",
    "$$\n",
    "\n",
    "This is the **proportion of neighbors** among the $k$ nearest that belong to class $j$.\n",
    "\n",
    "---\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. Compute distances from $x$ to all training points (e.g., Euclidean).\n",
    "2. Identify the $k$ closest points.\n",
    "3. Return the **majority class** or the **class with highest estimated probability**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Decision Boundary\n",
    "\n",
    "- The **decision boundary** is where the predicted class label changes.\n",
    "- For small $k$, the boundary is highly sensitive to local data and may be irregular.\n",
    "- Larger $k$ smooths the boundary by averaging over more neighbors.\n",
    "\n",
    "k-NN does not learn a function during training—it stores data and makes decisions only at prediction time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d7288",
   "metadata": {},
   "source": [
    "# Python Notes:\n",
    "\n",
    "The Python Tips in This Section Revolve around x y and z"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
